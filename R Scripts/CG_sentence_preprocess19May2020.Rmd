---
title: "CG_Text_Processing"
author: "E.T. Moseley"
date: "5/3/2020"
output: html_document
---

## Utility functions

#### clean_text

`clean_text()` accept a string of text, tokens, as well as a boolean, printout. It will remove carriage returns, remove text obfuscations, and convert the text to lowercase. If printout is`TRUE`, it will print out example text resulting from the removal of the above.

```{r}
clean_text <- function(tokens, printout){
    #Create a fake patient note phrase as a representative sample
    ex_token <- "Example note:\nThe patient is a 81yo m who was found down in [** location **] on [** date **] by daughter, [** name **].\n Pt was in usual state of health until four days ago, when began to complain to family of heartburn for which the pt was taking tums."
  if (printout){
    cat(ex_token, '\n')
  }
  
  ## Try removing carriage ANY returns before punctuation
    
#[1] "Example note"
#[2] "The patient is a 81yo m who was found down in location on date by daughter name"
#[3] "Pt was in usual state of health until four days ago when began to complain to family of heartburn for which the pt was taking tums"

  ##Remove carriage returns, convert to lower
  #tokens <- tolower(gsub('\r', ' ', tokens))
  #tokens <- tolower(gsub('\n', ' ', tokens))
  #ex_token <- tolower(gsub('\n', ' ', ex_token))
  #if (printout){
  #  cat("Result after removing carriage returns:\n")
  #  print(substr(ex_token, 1, 100))
  #}
  
  #https://stackoverflow.com/questions/13529360/replace-text-within-parenthesis-in-r
  #Remove obfuscations between '[' and ']'
  tokens <- gsub(" *\\[.*?\\] *", '\\1', tokens)
  ex_token <- gsub(" *\\[(.*?)\\] *", '\\1', ex_token)
  if (printout){
    print("Result after leaving [obfuscation]:\n")
    cat(ex_token, '\n')
  }
  
  ## Keep only words & numeric, but leave periods
   tokens <- gsub("(?!\\.)[[:punct:]]", "", tokens, perl=TRUE)
   ex_token <- gsub("(?!\\.)[[:punct:]]", "", ex_token, perl=TRUE)
   if (printout){
       print("Result after removing all but alphanumeric and spaces:\n")
       cat(ex_token, '\n')
    }
  
  ## Remove numeric
  ## tokens <- gsub("[[:digit:]]+", "", tokens, perl=TRUE)
  ## ex_token <- gsub("[[:digit:]]+", "", ex_token, perl=TRUE)
  ## if (printout){
##       print("Result after removing all but alphanumeric and spaces:\n")
##       cat(ex_token, '\n')
##  }
  
  #Keep only a single white space
  #https://stackoverflow.com/questions/25707647/merge-multiple-spaces-to-single-space-remove-trailing-leading-spaces
  tokens <- gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", '', tokens, perl=TRUE)
  ex_token <- gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", '', ex_token, perl=TRUE)
  if (printout){
    print("Result after keeping only single spaces:\n")
    cat(ex_token, '\n')
  }
  
    
   ## Keep only words & numeric, but leave periods
   tokens <- gsub(" \\.", ".", tokens, perl=TRUE)
   ex_token <- gsub(" \\.", ".", ex_token, perl=TRUE)
   if (printout){
       print("Result after removing all but alphanumeric and spaces:\n")
       cat(ex_token, '\n')
   }
  
#    txt <- tokenizers::tokenize_regex(dat$note_text, 
#                                  pattern = "\\.|\n",
#                                  simplify = FALSE)
   
#  print(tokenizers::tokenize_regex(ex_token, pattern = "\\.|\n", simplify = FALSE))
  ## Note: gsub("\\[QUOTED\\]", '"', dat$note_text[353])

  ## Trim white space and convert to lower
  return(trimws(tolower(tokens), "both"))
}
```

## Load Data

```{r}
dat <- read.csv("~/Desktop/CG_Data/caregivers_annotations19May2020.csv", 
                header = T, stringsAsFactors = F)
```

## Replace empty observations with "None"

```{r}
## Replace empty observations with "None"
dat$label_var[dat$label_var == ""] <- "None"

## Replace 0 text strings with ""
dat$text_string <- ifelse(dat$text_string == "0", "", dat$text_string)

## Ensure substring indices are accurate
## head(substr(dat$note_text, dat$first_index+1, dat$last_index+1), 15)
## head(dat$text_string, 15)

## Note that the carriage returns differ!!!
## Replace text with substrings!
dat$text_string <- substr(dat$note_text, dat$first_index+1, dat$last_index+1)

## table(dat$file_id, dat$labs)

as.data.frame(table(dat$label_var))
```


## Preprocess text

```{r}
dat$note_text <- clean_text(dat$note_text, FALSE)
dat$text_string <- clean_text(dat$text_string, FALSE)

## Remove variables which won't be used
## ID refers to the precise annotation id
dat$id <- NULL
#dat$first_index <- NULL
#dat$last_index <- NULL

## "has 3 children"

## dat[dat$text_string == "has children", ]
head(dat)
```

## Generate sentence-level data

Create two data frames-- one for note texts split and file ID, and the other with substrings, label, and ID

### Note Text

```{r}
txt <- tokenizers::tokenize_regex(dat$note_text, 
                                  pattern = "\\.|\n",
                                  simplify = FALSE)

lab <- tokenizers::tokenize_regex(dat$text_string,
                                  pattern = "\\.|\n",
                                  simplify = FALSE)

res <- data.frame()

for (i in 1:nrow(dat)){
    ## Initial subset of text and label data
    note_text <- txt[[i]]
    
    search_term <- lab[[i]]
    
    ## Store note ID info
    note_id <- rep(dat$file_id[i], each = length(note_text))
    
    ## Empty label_var for refilling
    label_var <- rep("", each = length(note_text))
    
    if (length(search_term) != 0){
        for (j in 1:length(search_term)){
            ## Label var i from data frame, label j from search_term
            tmp <- grepl(search_term[j], note_text)
            ## Use matrix properties
            label_var <- ifelse(tmp, dat$label_var[i], label_var)
        
        }
    }
    
    ## Bind results
    res <- rbind(res, cbind(note_id, note_text, label_var))
    
}
## Ensure results are character vectors
for (nm in colnames(res)) res[[nm]] <- as.character(res[[nm]])

res$label_var <- ifelse(res$label_var == "", "None", res$label_var)


rm(i, j, label_var, note_id, search_term, tmp, note_text, lab, txt, nm)

## Remove NAs resulting from splitting
res <- na.omit(res)

head(res[res$label_var == "Spouse/Partner",], 25)
```


```{r, echo = F, eval = F}
## Initial Preprocessing

### Check note text

## Count characters
res$note_chars <- nchar(res$note_text)

## Look at labels for low-char counts
## table(res[res$note_chars <= 2,]$label_var)

## boxplot(res$note_chars ~ res$label_var, horizontal = T)

## Look at text
## table(res[res$note_chars <= 2,]$note_text)

## Remove observations with 2 or less characters
## res <- res[res$note_chars > 2, ]

## Remove column
res$note_chars <- NULL
```


## Remove some noise in an attempt to balance the data set

Strategy:

1. Subset out `HCP` tokens
2. Bootstrap from `None` sample
3. Replace `HCP` observations

```{r}
## Save res just in case
holder <- res
```

```{r}
res <- holder

data_balance <- function(data, label_variable){
    
    tmp_res <- data.frame()

    for (i in 1:5){
        set.seed(i)
        ## Sample 25 percent
        tmp_res <- res[sample(nrow(res), round((nrow(res)*0.25), 0), replace = FALSE), ]
        
        ## Keep only "None" observations
        tmp_res <- tmp_res[tmp_res$label_var == "None", ]
        
        ## Remove any "None" sentences from data set
        res <- res[!(res$note_text %in% tmp_res$note_text), ]
        
        ## Print Results
        ## print("Begin:------")
        ## print(table(res$label_var))
    }
    
    ## Replace Observations observations
    res <- rbind(res[res$label_var == "None",], holder[holder$label_var == label_variable, ])

    colnames(res) <- c("sentence_source", "sentence", "label")
    
    ## Randomize results
    set.seed(1337)
    res <- res[sample(nrow(res)),]
    
    ## Return
    return(as.data.frame(res))
}

res <- data_balance(res, "Spouse/Partner")

## Sentence level balance
table(res$label)
```


```{r}
tmp <- as.data.frame(table(res$sentence_source, res$label))

tmp <- merge(tmp[tmp$Var2 == "None",], tmp[tmp$Var2 != "None",], by = "Var1")

## Remove NONE observations
tmp$Var2.x <- NULL
tmp$Freq.x <- NULL
tmp$Var2.y <- NULL

## Convert to binary
tmp$Freq.y <- ifelse(tmp$Freq.y >= 1, 1, 0)

## Change column names
colnames(tmp) <- c("sentence_source", "label")

## Data set balance
round(table(tmp$label)/nrow(tmp)*100, 2)

## Training/Testing Split
set.seed(1337)
train <- tmp[sample(nrow(tmp), round((nrow(tmp)*0.75), 0), replace = FALSE), ]
test <- tmp[!(tmp$sentence_source %in% train$sentence_source), ]

table(train$label)
table(test$label)
```

## Train/Test split then write

```{r}
## Convert label to binary
res$label <- ifelse(res$label == "None", 0, 1)

res_train <- res[res$sentence_source %in% train$sentence_source,]
res_test <- res[res$sentence_source %in% test$sentence_source,]

write.csv(res_train, file = "~/Desktop/CG_Data/learning_curves/balanced_spouse_train19May2020.csv", row.names = F)
write.csv(res_test, file = "~/Desktop/CG_Data/learning_curves/balanced_spouse_test19May2020.csv", row.names = F)
```



# LEARNING CURVE FILES

```{r}
## Holder for the files out
out_files <- vector()

for (nm in unique(res_train$sentence_source)){
    out_files <- c(out_files, nm)
    ## Subset sentences in each file
    tmp <- res_train[res_train$sentence_source %in% out_files,]
    if (length(out_files) %% 10 == 0){
            write.csv(tmp, file = paste("~/Desktop/CG_Data/learning_curves/spouse_train_", length(out_files), '_notes.csv', sep = ''), row.names=F)
    }
}

## Final write
write.csv(tmp, file = paste("~/Desktop/CG_Data/learning_curves/spouse_train_", length(out_files), '_notes.csv', sep = ''), row.names=F)
```







































## Data Set Analysis

```{r}
length(unique(res$sentence_source))
length(unique(dat$file_id))
## table(tmp$string_lab)
```

### Clean and write

```{r}
## Set seed
res$index <- 1:nrow(res)

set.seed(1337)
train <- res[sample(nrow(res), round((nrow(res)*0.75), 0), replace = FALSE), ]

test <- res[!(res$index %in% train$index), ]


train$index <- NULL
test$index <- NULL

table(train$label)
table(test$label)



## write.csv(train, file = "~/Desktop/CG_sentence_level_train04May2020.csv", row.names = F)
## write.csv(test, file = "~/Desktop/CG_sentence_level_test04May2020.csv", row.names = F)
```










## Write for specific data

```{r}
parse_data <- function(data, which_set){
    ## Change labels
    data$label <- tolower(gsub('\\s', '_', data$label))
    ## Replace /
    data$label <- gsub('/', '_', data$label)

    for (nm in names(table(data$label))){
        
        ## Make a temporary frame
        tmp_frame <- data
        ## Replace values based on label
        tmp_frame$label <- ifelse(tmp_frame$label == nm, 1, 0)
        
        print(nm)
        
        print(table(tmp_frame$label))
        
        print(paste("~/Desktop/CG_Data/",nm,'_',which_set,".csv", sep = ''))
        
        print("NEXT")
        
        write.csv(tmp_frame, file = paste("~/Desktop/CG_Data/",nm,'_',which_set,".csv", sep = ''), row.names = F)
    }
    
}

parse_data(train, "train")
parse_data(test, "test")
```

## Aggregate All Caregivers' Strings

```{r}
## Replace Code Status Limitations and Goals of Care variables with None for now
## All else gets "HCP"
res$label <- ifelse(res$label %in% c("Code Status Limitations", 
                                             "Goals of Care",
                                             "None"), "None", "HCP")


## Set seed
res$index <- 1:nrow(res)

set.seed(1337)
train <- res[sample(nrow(res), round((nrow(res)*0.75), 0), replace = FALSE), ]

test <- res[!(res$index %in% train$index), ]


train$index <- NULL
test$index <- NULL

table(train$label)
table(test$label)


parse_data(train, "train")
parse_data(test, "test")
```