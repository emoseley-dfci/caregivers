---
title: "Caregivers Initial Cohort Prep & Regex"
output: html_document
---

### Hypotheses:

1. Family members are more likely to have documented discussions about mechanical ventilation in the intensive care setting when the patient SOFA score greater than 11, Elixhauser Comorbidity Index score of XX or higher, dialysis support is present, and vasopressor support is present. patient is experiencing sepsis, ventilation-acquired pneumonia, or organ failure.

2. Older patient age, longer ICU length of stay, and higher mortality rate will be associated with family involvement in discussions about mechanical ventilation.

3. Dependent children of patients will be associated with more discussions about mechanical ventilation in the intensive care setting compared to spouses of patients.

### Strategy

#### Cohort

1. Load entire cohort
2. Subset MICU observations
3. Load additional mimic data: `SOFA`, `elixhauser`, `vasopressordurations`, `rrtfirstday`
4. Check if vasopressors were used within 48hrs of ICU intime

#### Note Data

1. Pull Spouse/Child labels from manual annotation
2. Preprocess for keywords, manual review top 10 for keywords with high sensitivity
3. Create dictionary of possible permutations of word (plural, etc.)
4. Run regex on all MICU notes (this mimics what ClinicalRegex does– would be useful to use ClinicalRegex’ Regular Expression pipeline– inc. lemmatization– but that may be difficult on short time)
5. Merge data, write.


```{r, echo = F}
## Rename columns, avoiding some
rename <- function(dat, avoid_names, prefix){
        for (name in colnames(dat)[!(colnames(dat) %in% avoid_names)]){
            colnames(dat)[which(colnames(dat) == name)] <- paste(prefix, name, sep = '')
        }
    return(dat)
}

clean_text <- function(tokens, printout){
    #Create a fake patient note phrase as a representative sample
    ex_token <- "Example note:\nThe patient is a 81yo m who was found down in [** location **] on [** date **] by daughter, [** name **].\n Pt was in usual state of health until four days ago, when began to complain to family of heartburn for which the pt was taking tums."
  if (printout){
    cat(ex_token, '\n')
  }

  ##Remove carriage returns, convert to lower
  tokens <- tolower(gsub('\r', ' ', tokens))
  tokens <- tolower(gsub('\n', ' ', tokens))
  ex_token <- tolower(gsub('\n', ' ', ex_token))
  if (printout){
    cat("Result after removing carriage returns:\n")
    print(substr(ex_token, 1, 100))
  }
  
  #https://stackoverflow.com/questions/13529360/replace-text-within-parenthesis-in-r
  #Remove obfuscations between '[' and ']'
  tokens <- gsub(" *\\[.*?\\] *", '\\1', tokens)
  ex_token <- gsub(" *\\[(.*?)\\] *", '\\1', ex_token)
  if (printout){
    print("Result after leaving [obfuscation]:\n")
    cat(ex_token, '\n')
  }
  
  ## Keep only words & numeric
   tokens <- gsub("[[:punct:]]", "", tokens, perl=TRUE)
   ex_token <- gsub("[[:punct:]]", "", ex_token, perl=TRUE)
   if (printout){
       print("Result after removing all but alphanumeric and spaces:\n")
       cat(ex_token, '\n')
    }
  
  ## Remove numeric
  tokens <- gsub("[[:digit:]]+", "", tokens, perl=TRUE)
  ex_token <- gsub("[[:digit:]]+", "", ex_token, perl=TRUE)
  if (printout){
       print("Result after removing all but alphanumeric and spaces:\n")
       cat(ex_token, '\n')
  }
  
  #Keep only a single white space
  #https://stackoverflow.com/questions/25707647/merge-multiple-spaces-to-single-space-remove-trailing-leading-spaces
  tokens <- gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", '', tokens, perl=TRUE)
  ex_token <- gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", '', ex_token, perl=TRUE)
  if (printout){
    print("Result after keeping only single spaces:\n")
    cat(ex_token, '\n')
  }
  
    
   ## Keep only words & numeric, but leave periods
   tokens <- gsub(" \\.", ".", tokens, perl=TRUE)
   ex_token <- gsub(" \\.", ".", ex_token, perl=TRUE)
   if (printout){
       print("Result after removing all but alphanumeric and spaces:\n")
       cat(ex_token, '\n')
   }
  
  return(trimws(tolower(tokens), "both"))
}

#Split strings on ' ', unlist, tabulate
prep <- function(dat){
  tmp <- as.data.frame(table(unlist(strsplit(dat, ' '))))
  tmp <- tmp[rev(order(tmp$Freq)),]
  return(tmp)
}

## Regex Utility Function
## strictRegex() will accept all phrases kwds, and all note texts, texts, it will utilize grepl() to find direct matches in the text, and will return a list of booleans.

strictRegex <- function(kwds, texts){
  #Create a list to store results
  tmpList <- list()
  
  #Loop through all keywords
  for (i in 1:length(kwds)){
    #Store results as a logical vector in its respective list entry position
    tmpList[[i]] <- grepl(kwds[i], texts, ignore.case = TRUE)
  }
  
  #Return list and control to environment
  return(tmpList)
}

## Convert output to data frame
to_df <- function(domain, rule){
  #Convert list from grepl to data frame
  domain <- as.data.frame(domain)
  #Show column names as phrases
  colnames(domain) <- rule[rule != '']
  #Multiply by 1 for binary numeric
  domain <- domain*1
  return(domain)
}

## Collapse phrases to their domains
phrase_to_domain <- function(dat){
  inc <- vector()
  for (i in 1:nrow(dat)){
    #Collapse phrases by domain, presence of any phrase indicates domain
    inc[i] <- any(dat[i,] == 1)
  }
  
  #Multiply by one to convert logical to binary numeric
  return(inc*1)
}
```

## Cohort Data

```{r}
## Load entire cohort
tmp <- read.csv("~/Desktop/CG_Data/initial_cohort05Nov19.csv", header = T, stringsAsFactors = F)

nrow(tmp)

## Keep only MICU observations
tmp <- tmp[tmp$FIRST_CAREUNIT == "MICU",]

nrow(tmp)
```

### Additional Tables

#### Renal Replacement Therapy on Day One

```{r}
rrtd1 <- read.csv("~/MIMIC-III/rrtfirstday.csv", header = T, stringsAsFactors = F)
colnames(rrtd1) <- toupper(colnames(rrtd1))
rrtd1 <- aggregate(RRT ~ ICUSTAY_ID, data = rrtd1, FUN = max)
colnames(rrtd1) <- c("ICUSTAY_ID", "RRT_D1")

## Merge
tmp <- merge(x = tmp, y = rrtd1, by = "ICUSTAY_ID", all.x = TRUE)

rm(rrtd1)
```

#### Sequential Organ Failure Assessment (SOFA)

```{r}
sofa <- read.csv("~/MIMIC-III/sofa.csv", header = T, stringsAsFactors = F)
colnames(sofa) <- toupper(colnames(sofa))

## Subset
sofa <- sofa[, c("SUBJECT_ID", "HADM_ID", "ICUSTAY_ID", "SOFA")]
sofa <- rename(sofa, c("SUBJECT_ID", "HADM_ID", "ICUSTAY_ID"), "ICU_D1_")

## Merge
tmp <- merge(x = tmp, y = sofa, by = c("SUBJECT_ID", "HADM_ID", "ICUSTAY_ID"), all.x = TRUE)
rm(sofa)
```

#### Elixhauser Comorbidities

```{r}
elixhauser <- read.csv("~/MIMIC-III/elixhauser.csv", header = T, stringsAsFactors = F)
colnames(elixhauser) <- toupper(colnames(elixhauser))
elixhauser <- rename(elixhauser, "HADM_ID", "HOSP_ADMIT_")

## Merge
tmp <- merge(x = tmp, y = elixhauser, by = "HADM_ID", all.x = TRUE)

## Clean
rm(elixhauser)
```

#### Vasopressors

```{r}
vaso <- read.csv("~/MIMIC-III/vasopressordurations.csv", header = T, stringsAsFactors = F)
colnames(vaso) <- toupper(colnames(vaso))
vaso <- rename(vaso, "ICUSTAY_ID", "VP_")
## Drop vasopressor count
vaso$VASONUM <- NULL

## Temporary merge for calculations
tmp_two <- merge(tmp[,c("ICUSTAY_ID", "INTIME")], vaso, by = "ICUSTAY_ID", all.x = TRUE)

## See if vasopressors were required within this 48hr timeframe since ICU intime
tmp_two$VASOPRESSORS <- ifelse((((as.POSIXlt(as.character(tmp_two$VP_ENDTIME), tz = "EST"))) >= (as.POSIXlt(as.character(tmp_two$INTIME), tz = "EST") + (2 * (24 * 60 * 60) ))), 1, 0)

## Correct NAs to 0
tmp_two$VASOPRESSORS <- ifelse(is.na(tmp_two$VASOPRESSORS), 0, tmp_two$VASOPRESSORS)

tmp_two <- aggregate(VASOPRESSORS ~ ICUSTAY_ID, data = tmp_two, FUN = max)

## Merge back to data
tmp <- merge(x = tmp, y = tmp_two, by = c("ICUSTAY_ID"), all.x = TRUE)

## Clean
rm(vaso, tmp_two)
```

## Load Spouse/Child Annotation Data

```{r}
dat <- read.csv("~/Downloads/caregivers_annotations26May2020.csv", header = T, stringsAsFactors = F)
## Spouse/Child labels only
dat <- dat[dat$label_var %in% c("Child", "Spouse/Partner"),]
```

#### Utility Functinos

## Clean text and Create library of Tokens

Create library of tokens in each category
For work-level statistics we will split our phrases on spaces to generate individual tokens. We will store them in each respective variable.

```{r}
## Clean cohort note text and results text string
tmp$TEXT <- clean_text(tmp$TEXT, F)
dat$text_string <- clean_text(dat$text_string, F)

child <- prep(dat[dat$label_var == "Child",]$text_string)
spouse <- prep(dat[dat$label_var == "Spouse/Partner",]$text_string)

## Note the high frequency of possesive language (his, hers, pt's)
head(cbind(child, spouse), 10)
```

#### Dictionary of tokens selected from top 10

Note: correcting here for plural

```{r}
child <- c("daughter", "daughters", "son", "sons")
spouse <- c("wife", "wifes", "husband", "husbands")
```

## Perform regex (optimizing on sensitivity)

```{r}
## Regex on entire cohort text
chl <- strictRegex(child, tmp$TEXT)
sps <- strictRegex(spouse, tmp$TEXT)

chl <- to_df(chl, child)
sps <- to_df(sps, spouse)

res <- cbind(phrase_to_domain(chl),
             phrase_to_domain(sps))

colnames(res) <- c("CHILD", "SPOUSE")

res <- as.data.frame(res)

## merge data
dat <- cbind(tmp, res)
## Remove note text from dat for ease
dat$TEXT <- NULL
```

## Quick Check

```{r}
summary(dat[dat$CHILD == 1,]$CHART_HRS_SINCE_ADMIT)
summary(dat[dat$SPOUSE == 1,]$CHART_HRS_SINCE_ADMIT)
```

#### Hypothesis Test on the fly

```{r}
## Non-parametric Hypothesis test
wilcox.test(dat[dat$CHILD == 1,]$CHART_HRS_SINCE_ADMIT, dat[dat$SPOUSE == 1,]$CHART_HRS_SINCE_ADMIT)

## T test for easier interpretation (assumes normality)
t.test(dat[dat$CHILD == 1,]$CHART_HRS_SINCE_ADMIT, dat[dat$SPOUSE == 1,]$CHART_HRS_SINCE_ADMIT)
```

Back-of-envelope analysis seems to suggest documented discussions involving children are comming in at a statistically significant ~1.5hrs later than documented discussions involving spouses.

### Write

```{r}
write.csv(dat, file = "~/Desktop/CG_Data/initial_full_regex_results27May2020.csv", row.names = F)
```