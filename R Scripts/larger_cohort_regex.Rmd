---
title: "large_cohort_regex"
output: html_document
---


```{r, echo = F}
## Rename columns, avoiding some
rename <- function(dat, avoid_names, prefix){
        for (name in colnames(dat)[!(colnames(dat) %in% avoid_names)]){
            colnames(dat)[which(colnames(dat) == name)] <- paste(prefix, name, sep = '')
        }
    return(dat)
}

clean_text <- function(tokens, printout){
    #Create a fake patient note phrase as a representative sample
    ex_token <- "Example note:\nThe patient is a 81yo m who was found down in [** location **] on [** date **] by daughter, [** name **].\n Pt was in usual state of health until four days ago, when began to complain to family of heartburn for which the pt was taking tums."
  if (printout){
    cat(ex_token, '\n')
  }

  ##Remove carriage returns, convert to lower
  tokens <- tolower(gsub('\r', ' ', tokens))
  tokens <- tolower(gsub('\n', ' ', tokens))
  ex_token <- tolower(gsub('\n', ' ', ex_token))
  if (printout){
    cat("Result after removing carriage returns:\n")
    print(substr(ex_token, 1, 100))
  }
  
  #https://stackoverflow.com/questions/13529360/replace-text-within-parenthesis-in-r
  #Remove obfuscations between '[' and ']'
  tokens <- gsub(" *\\[.*?\\] *", '\\1', tokens)
  ex_token <- gsub(" *\\[(.*?)\\] *", '\\1', ex_token)
  if (printout){
    print("Result after leaving [obfuscation]:\n")
    cat(ex_token, '\n')
  }
  
  ## Keep only words & numeric
   tokens <- gsub("[[:punct:]]", "", tokens, perl=TRUE)
   ex_token <- gsub("[[:punct:]]", "", ex_token, perl=TRUE)
   if (printout){
       print("Result after removing all but alphanumeric and spaces:\n")
       cat(ex_token, '\n')
    }
  
  ## Remove numeric
  tokens <- gsub("[[:digit:]]+", "", tokens, perl=TRUE)
  ex_token <- gsub("[[:digit:]]+", "", ex_token, perl=TRUE)
  if (printout){
       print("Result after removing all but alphanumeric and spaces:\n")
       cat(ex_token, '\n')
  }
  
  #Keep only a single white space
  #https://stackoverflow.com/questions/25707647/merge-multiple-spaces-to-single-space-remove-trailing-leading-spaces
  tokens <- gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", '', tokens, perl=TRUE)
  ex_token <- gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", '', ex_token, perl=TRUE)
  if (printout){
    print("Result after keeping only single spaces:\n")
    cat(ex_token, '\n')
  }
  
    
   ## Keep only words & numeric, but leave periods
   tokens <- gsub(" \\.", ".", tokens, perl=TRUE)
   ex_token <- gsub(" \\.", ".", ex_token, perl=TRUE)
   if (printout){
       print("Result after removing all but alphanumeric and spaces:\n")
       cat(ex_token, '\n')
   }
  
  return(trimws(tolower(tokens), "both"))
}

#Split strings on ' ', unlist, tabulate
prep <- function(dat){
  tmp <- as.data.frame(table(unlist(strsplit(dat, ' '))))
  tmp <- tmp[rev(order(tmp$Freq)),]
  return(tmp)
}

## Regex Utility Function
## strictRegex() will accept all phrases kwds, and all note texts, texts, it will utilize grepl() to find direct matches in the text, and will return a list of booleans.

strictRegex <- function(kwds, texts){
  #Create a list to store results
  tmpList <- list()
  
  #Loop through all keywords
  for (i in 1:length(kwds)){
    #Store results as a logical vector in its respective list entry position
    tmpList[[i]] <- grepl(kwds[i], texts, ignore.case = TRUE)
  }
  
  #Return list and control to environment
  return(tmpList)
}

## Convert output to data frame
to_df <- function(domain, rule){
  #Convert list from grepl to data frame
  domain <- as.data.frame(domain)
  #Show column names as phrases
  colnames(domain) <- rule[rule != '']
  #Multiply by 1 for binary numeric
  domain <- domain*1
  return(domain)
}

## Collapse phrases to their domains
phrase_to_domain <- function(dat){
  inc <- vector()
  for (i in 1:nrow(dat)){
    #Collapse phrases by domain, presence of any phrase indicates domain
    inc[i] <- any(dat[i,] == 1)
  }
  
  #Multiply by one to convert logical to binary numeric
  return(inc*1)
}
```


## Load Data

```{r}
dat <- read.csv("~/Desktop/CG_Data/caregivers_micu_noteevents02Jun20.csv", header = T, stringsAsFactors = F)
```

## Clean text before regex

```{r}
## Save text in NOTE_TEXT
dat$NOTE_TEXT <- dat$TEXT

dat$TEXT <- clean_text(dat$TEXT, printout = F)
```

## Keyword libraries

1. Clean as with other text
2. Add Spaces before and after

Note: spaces used so as not to capture things like `"Parkinsons"` with `"son"`.

```{r, warning=F}
child <- paste('', unique(clean_text(readLines("~/Desktop/CG_Data/caregivers/child_phrase_dictionary.txt"), F)), '', sep = ' ')

## Remove yo, as it is used frequently
child <- child[!(child %in% c(" yo ", " yos "))]

spouse <- paste('', unique(clean_text(readLines("~/Desktop/CG_Data/caregivers/spouse_phrase_dictionary.txt"), F)), '', sep = ' ')
```

## Perform regex (optimizing on sensitivity)

```{r}
## Regex on entire cohort text
chl <- strictRegex(child, dat$TEXT)
sps <- strictRegex(spouse, dat$TEXT)

chl <- to_df(chl, child)
sps <- to_df(sps, spouse)

res <- cbind(phrase_to_domain(chl),
             phrase_to_domain(sps))

colnames(res) <- c("CHILD", "SPOUSE")

res <- as.data.frame(res)

## merge data
dat <- cbind(dat, res)

## Replace note text with messy note text
dat$TEXT <- dat$NOTE_TEXT
dat$NOTE_TEXT <- NULL

rm(chl, sps, res)
```

## Quick Check

```{r}
print(sum(dat$CHILD)/nrow(dat)*100)
print(sum(dat$SPOUSE)/nrow(dat)*100)
```

#### Hypothesis Test on the fly

```{r}
## Non-parametric Hypothesis test
wilcox.test(dat[dat$CHILD == 1,]$CHART_HRS_SINCE_ADMIT, dat[dat$SPOUSE == 1,]$CHART_HRS_SINCE_ADMIT)
```

### Write

```{r}
write.csv(dat, file = "~/Desktop/CG_Data/caregivers/investigational_regex_results03Jun2020.csv", row.names = F)
```