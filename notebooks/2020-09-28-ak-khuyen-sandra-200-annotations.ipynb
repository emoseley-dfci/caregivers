{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAA Quick Check\n",
    "\n",
    "Khuyen and Sandra, first 200 notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_K = '../data/interim/2020-09-22-kd-200-annotations.csv'\n",
    "FILE_S = '../data/interim/2020-09-28-sz-200-annotations.csv'\n",
    "FILE_original = '../data/interim/caregivers_set13Jul2020.csv'\n",
    "\n",
    "N_ROWS_TO_USE = 234                        # see note below on why this is\n",
    "\n",
    "df_o = pd.read_csv(FILE_original)          # original file provided to annotators, in case it's helpful later\n",
    "df_k = pd.read_csv(FILE_K)\n",
    "df_s = pd.read_csv(FILE_S)\n",
    "\n",
    "ids_k = df_k[:N_ROWS_TO_USE]['id']\n",
    "ids_s = df_s[:N_ROWS_TO_USE]['id']\n",
    "\n",
    "ids = set(ids_k).intersection(set(ids_s))  # common annotations, in theory ids == ids_k == ids_s and len(ids) == 200\n",
    "\n",
    "assert ids_k.equals(ids_s)\n",
    "assert len(ids) == 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on Clinical Regex output peculiarity\n",
    "The rows in the imported DataFrame don't actually correspond to the entry numbers displayed when using Clinical Regex. \n",
    "\n",
    "For example, Khuyen's file loaded into Clinical Regex indicates she stopped at entry 201, which has `id=105582` (`id` here corresponds to `HADM_ID` in the original data file). But `id=105582` is at index 234 in the DataFrame (see below).\n",
    "\n",
    "For now I will assume the set of unique values in `all_ids[:235]` is the set which was annotated. (The size of this set here does indeed turn out to be 200, so it's plausible that this is the case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the final id within the imported DataFrame is 234.\n"
     ]
    }
   ],
   "source": [
    "# I use Khuyen's data for this, but it's easy to check it's the same case for Sandra's as well\n",
    "\n",
    "ids_unique = df_k['id'].unique()\n",
    "ids_all = df_k['id']\n",
    "\n",
    "id_final = 105582\n",
    "id_final_index = list(ids_all).index(id_final)\n",
    "\n",
    "print('The index of the final id within the imported DataFrame is {}.'.format(id_final_index))\n",
    "\n",
    "assert list(ids_unique).index(id_final) == 200\n",
    "assert len(ids_all[:id_final_index].unique()) == 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minor Cleanup\n",
    "\n",
    "- Rehsape annotation data (annotations are provided in a pseudo-JSON string column)\n",
    "- Resolve rows with duplicate `id`s\n",
    "- Select rows that were annotated by both annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_cr_json(df):\n",
    "    \"\"\"Convert ClinicalRegex's 'annotationValues' from a 'JSON' string\n",
    "    (into appropriate columns).\"\"\"\n",
    "    df_annotations = pd.json_normalize(df['annotationValues'].map(json.loads))\n",
    "    \n",
    "    return pd.concat([df, df_annotations], axis=1)\n",
    "\n",
    "def resolve_annotations(arr, binary=False):\n",
    "    \"\"\"An entry (in this case this is a HADM) gets a value of 1 if\n",
    "    any of the notes are annotated with 1.\n",
    "    \n",
    "    I provide a binary option here to be able to treat the '9's as '1's.\n",
    "    \"\"\"\n",
    "    \n",
    "    if binary:\n",
    "        arr = [val if val != 9 else 1 for val in arr]\n",
    "    \n",
    "    unique = set(arr)\n",
    "    \n",
    "    if len(arr) <= 0:\n",
    "        return arr\n",
    "\n",
    "    elif len(unique) == 1:\n",
    "        return list(arr)[0]\n",
    "    \n",
    "    elif unique == {0, 1}:\n",
    "        return 1\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('Resolution of annotations unclear.', arr)\n",
    "        \n",
    "def get_annotation_values(df, binary=False):\n",
    "    \"\"\"\"\"\"\n",
    "    return df.groupby('id')\\\n",
    "             .agg({'L1_annotation': lambda v: resolve_annotations(v, binary),\n",
    "                   'L2_annotation': lambda v: resolve_annotations(v, binary),\n",
    "                   'L3_annotation': lambda v: resolve_annotations(v, binary)})\\\n",
    "             .reset_index()\n",
    "\n",
    "def select_annotated_rows(df):\n",
    "    \"\"\"Take the subset of common annotations.\"\"\"\n",
    "    return df[df['id'].isin(ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# raw annotations\n",
    "df_k_raw = df_k.pipe(reshape_cr_json)\\\n",
    "               .pipe(get_annotation_values)\\\n",
    "               .pipe(select_annotated_rows)\n",
    "df_s_raw = df_s.pipe(reshape_cr_json)\\\n",
    "               .pipe(get_annotation_values)\\\n",
    "               .pipe(select_annotated_rows)\n",
    "\n",
    "# annotations if 9 is resolved to 1\n",
    "df_k_res = df_k.pipe(reshape_cr_json)\\\n",
    "               .pipe(get_annotation_values, binary=True)\\\n",
    "               .pipe(select_annotated_rows)\n",
    "df_s_res = df_s.pipe(reshape_cr_json)\\\n",
    "               .pipe(get_annotation_values, binary=True)\\\n",
    "               .pipe(select_annotated_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohen Kappa Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label 1: Child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8299167487243756"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(df_k_raw['L1_annotation'],\n",
    "                  df_s_raw['L1_annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8779405237461163"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(df_k_res['L1_annotation'],\n",
    "                  df_s_res['L1_annotation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label 2: Spouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181893310193793"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(df_k_raw['L2_annotation'],\n",
    "                  df_s_raw['L2_annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8984513835998984"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(df_k_res['L2_annotation'],\n",
    "                  df_s_res['L2_annotation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix / Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L1_annotation</th>\n",
       "      <th>L2_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   L1_annotation  L2_annotation\n",
       "1            116            105\n",
       "0             66             77\n",
       "9             18             18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_k_raw[['L1_annotation', 'L2_annotation']].apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L1_annotation</th>\n",
       "      <th>L2_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   L1_annotation  L2_annotation\n",
       "1            111             87\n",
       "0             67             80\n",
       "9             20             28\n",
       "5              2              5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s_raw[['L1_annotation', 'L2_annotation']].apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 62,   4,   0,   0],\n",
       "       [  3, 105,   2,   6],\n",
       "       [  0,   0,   0,   0],\n",
       "       [  2,   2,   0,  14]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_k_raw['L1_annotation'],\n",
    "                 df_s_raw['L1_annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76,  1,  0,  0],\n",
       "       [ 4, 85,  5, 11],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  1,  0, 17]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df_k_raw['L2_annotation'],\n",
    "                 df_s_raw['L2_annotation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
